@Manual{PCADSC,
    title = {PCADSC: Tools for Principal Component Analysis-Based Data Structure
Comparisons},
    author = {Anne H. Petersen and Bo Markussen},
    year = {2017},
    note = {R package version 0.8.0},
    url = {https://CRAN.R-project.org/package=PCADSC},
}
  
@article {Abdi2010,
author = {Abdi, Hervé and Williams, Lynne J.},
title = {Principal component analysis},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
volume = {2},
number = {4},
publisher = {John Wiley & Sons, Inc.},
issn = {1939-0068},
url = {http://dx.doi.org/10.1002/wics.101},
doi = {10.1002/wics.101},
pages = {433--459},
keywords = {singular and eigen value decomposition, bilinear decomposition, factor scores and loadings, RESS PRESS, multiple factor analysis},
year = {2010},
}
 
@inbook{HastieEtAl2009,
author = {Hastie, T and Tibshirani, R and Friedman, J},
title = {The Elements of Statistical Learning},
chapter = {14.5.1},
publisher = {Springer},
year = {2009},
edition = {Second}
}

@article{AltmanBland83,
author = {Altman, D G and Bland, J M},
title = {{Measurement in medicine: the analysis of method comparison studies}},
journal = {The Statistician},
volume = {32},
year = {1983},
pages = {307-317}
}

@article{LinEtAl2002,
author = {Lin, D Y and Wei, L J and Ying, Z},
title = {{Model-Checking Techniques Based on Cumulative Residuals}},
journal = {Biometrics},
year = {2002},
volume = {58},
pages = {1-12}
}

@article{ZwitserEtAl2017,
author = {Zwitser, R J and Glaser, S S F and Maris, G},
title = {{Monitoring Countries in a Changing World: A New Look at DIF in International Surveys}},
journal = {Psychometrika},
year = {2017},
volume = {82},
number = {1},
pages = {210-232}
}

@article{Liu2016,
author = {Mingnan Liu},
title = {{Comparing data quality between online panel and intercept samples}},
journal = {Methodological Innovations},
volume = {9},
number = {},
pages = {2059799116672877},
year = {2016},
doi = {10.1177/2059799116672877},
URL = {http://dx.doi.org/10.1177/2059799116672877},
eprint = {http://dx.doi.org/10.1177/2059799116672877},
abstract = { Although some research effort has been devoted to the comparison of probability- and nonprobability-based
    Web surveys, different types of nonprobability-based samples have not been thoroughly examined. This exploratory study
    compares the data quality between online panel and intercept samples. Online panel refers to a pre-recruited and
    profiled pool of respondents. An intercept sample is a pool of respondents that are obtained through banners, ads, or
    promotions. Anyone can click on them and subsequently respond to a survey. Respondents are not pre-recruited or
    profiled. Three surveys with 52, 29, and 19 questions, respectively, were administered to both samples. Propensity score
    weighting adjustment is used for the analyses. The results show that the completion rates are higher for the panel than
    the intercept sample. The completion times are similar for these two samples. Data quality, on average, tends to be
    higher for panel than intercept samples. }
}

@article{Feveile2007,
abstract = {BACKGROUND: Data for health surveys are often collected using either mailed questionnaires, telephone interviews
or a combination. Mode of data collection can affect the propensity to refuse to respond and result in different patterns of
responses. The objective of this paper is to examine and quantify effects of mode of data collection in health surveys.

METHODS: A stratified sample of 4,000 adults residing in Denmark was randomised to mailed questionnaires or
computer-assisted telephone interviews. 45 health-related items were analyzed; four concerning behaviour and 41 concerning
self assessment. Odds ratios for more positive answers and more frequent use of extreme response categories (both positive
and negative) among telephone respondents compared to questionnaire respondents were estimated. Tests were Bonferroni
corrected.

RESULTS: For the four health behaviour items there were no significant differences in the response patterns. For 32 of the
41 health self assessment items the response pattern was statistically significantly different and extreme response
categories were used more frequently among telephone respondents (Median estimated odds ratio: 1.67). For a majority of
these mode sensitive items (26/32), a more positive reporting was observed among telephone respondents (Median estimated
odds ratio: 1.73). The overall response rate was similar among persons randomly assigned to questionnaires (58.1{\%}) and to
telephone interviews (56.2{\%}). A differential nonresponse bias for age and gender was observed. The rate of missing
responses was higher for questionnaires (0.73-6.00{\%}) than for telephone interviews (0-0.51{\%}). The "don't know" option
was used more often by mail respondents (10-24{\%}) than by telephone respondents (2-4{\%}).

CONCLUSION: The mode of data collection affects the reporting of self assessed health items substantially. In
epidemiological studies, the method effect may be as large as the effects under investigation. Caution is needed when
comparing prevalences across surveys or when studying time trends.},
author = {Feveile, Helene and Olsen, Ole and Hogh, Annie},
doi = {10.1186/1471-2288-7-27},
issn = {1471-2288},
journal = {BMC medical research methodology},
keywords = {Adult,Attitude to Health,Consumer Participation,Denmark,Female,Health Behavior,Health Surveys,Humans,Interviews
as Topic,Male,Odds Ratio,Postal Service,Questionnaires,Research Design,Self-Assessment,Telephone},
month = {jan},
pages = {27},
pmid = {17592653},
title = {{A randomized trial of mailed questionnaires versus telephone interviews: response patterns in a survey.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1925106{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2007}
}

@article{Brambilla1987,
abstract = {This paper compares respondents to mailed questionnaires with those nonrespondents subsequently interviewed by
telephone in a survey of Massachusetts women aged 45-55 years conducted in 1981-1982. This mixed mode approach produced
8,050 responses, giving a response rate of 77{\%}. This rate is similar to rates obtained in many surveys that employed
in-person interviews, which are still widely used in health surveys but are increasingly expensive. Telephone respondents
differed socioeconomically from mail respondents, suggesting that telephone follow-up of nonrespondents may have reduced
nonresponse bias in this survey. Thus, a mixed mode approach may be superior to a mail-only approach with respect to this
aspect of data quality. Women responding by mail were more likely to hold professional jobs, to have relatively high
household incomes, and to have more years of education. Controlling for these socioeconomic differences did not, however,
remove differences in reported health outcomes between mail and telephone respondents. These differences may be explained by
less complete recall in the telephone interviews or they may arise from actual differences in health profiles between early
(i.e., mail) and late (i.e., telephone) respondents. Although a mixed mode approach may reduce nonresponse bias, more
research is required concerning the reasons for response differences between modes and to eliminate any differences caused
by problems in data quality.},
author = {Brambilla, D J and McKinlay, S M},
issn = {0002-9262},
journal = {American journal of epidemiology},
month = {nov},
number = {5},
pages = {962--71},
pmid = {3661543},
title = {{A comparison of responses to mailed questionnaires and telephone interviews in a mixed mode health survey.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3661543},
volume = {126},
year = {1987}
}

@article{McHorney1994,
abstract = {Many health status surveys have been designed for mail, telephone, or in-person administration. However, with
rare exception, investigators have not studied the effect the survey mode of administration has on the way respondents
assess their health and other important parameters (such as response rates, nonresponse bias, and data quality), which can
affect the generalizability of results. Using a national sampling frame of noninstitutionalized adults from the General
Social Survey, we randomly assigned adults to a mail survey (80{\%}) or a computer-assisted telephone survey (20{\%}). The
surveys were designed to provide national norms for the SF-36 Health Survey. Total data collection costs per case for the
telephone survey ({\$}47.86) were 77{\%} higher than that for the mail survey ({\$}27.07). A significantly higher response
rate was achieved among respondents randomly assigned to the mail (79.2{\%}) than telephone survey (68.9{\%}). Nonresponse
bias was evident in both modes but, with the exception of age, was not differential between modes. The rate of missing
responses was higher for mail than telephone respondents (1.59 vs. 0.49 missing items). Health ratings based on the SF-36
scales were less favorable, and reports of chronic conditions were more frequent, for mail than telephone respondents.
Results are discussed in light of the trade-offs involved in choosing a survey methodology for health status assessment
applications. Norms for mail and telephone versions of the SF-36 survey are provided for use in interpreting individual and
group scores.},
author = {McHorney, C A and Kosinski, M and Ware, J E},
issn = {0025-7079},
journal = {Medical care},
month = {jun},
number = {6},
pages = {551--67},
pmid = {8189774},
title = {{Comparisons of the costs and quality of norms for the SF-36 health survey collected by mail versus telephone
interview: results from a national survey.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8189774},
volume = {32},
year = {1994}
}

@article{Powers2005,
abstract = {OBJECTIVES To estimate differences in self-rated health by mode of administration and to assess the value of
multiple imputation to make self-rated health comparable for telephone and mail. METHODS In 1996, Survey 1 of the Australian
Longitudinal Study on Women's Health was answered by mail. In 1998, 706 and 11,595 mid-age women answered Survey 2 by
telephone and mail respectively. Self-rated health was measured by the physical and mental health scores of the SF-36. Mean
change in SF-36 scores between Surveys 1 and 2 were compared for telephone and mail respondents to Survey 2, before and
after adjustment for sociodemographic and health characteristics. Missing values and SF-36 scores for telephone respondents
at Survey 2 were imputed from SF-36 mail responses and telephone and mail responses to sociodemographic and health
questions. RESULTS At Survey 2, self-rated health improved for telephone respondents but not mail respondents. After
adjustment, mean changes in physical health and mental health scores remained higher (0.4 and 1.6 respectively) for
telephone respondents compared with mail respondents (-1.2 and 0.1 respectively). Multiple imputation yielded adjusted
changes in SF-36 scores that were similar for telephone and mail respondents. CONCLUSIONS AND IMPLICATIONS The effect of
mode of administration on the change in mental health is important given that a difference of two points in SF-36 scores is
accepted as clinically meaningful. Health evaluators should be aware of and adjust for the effects of mode of administration
on self-rated health. Multiple imputation is one method that may be used to adjust SF-36 scores for mode of administration
bias.},
author = {Powers, J R and Mishra, G and Young, A F},
issn = {1326-0200},
journal = {Australian and New Zealand journal of public health},
month = {apr},
number = {2},
pages = {149--54},
pmid = {15915619},
title = {{Differences in mail and telephone responses to self-rated health: use of multiple imputation in correcting for
response bias.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15915619},
volume = {29},
year = {2005}
}
@article{Kankaras2014,
author = {Kankara{\v{s}}, Milo{\v{s}} and Moors, Guy},
doi = {10.1177/0022022113511297},
journal = {Journal of Cross-Cultural Psychology},
year = {2014},
pages = {381-399},
volume = {45},
number = {3},
title = {{Analysis of Cross-Cultural Comparability of PISA 2009 Scores}}
}
@article{Kreiner2014,
author = {Kreiner, Svend and Christensen, Karl Bang},
doi = {10.1007/s11336-013-9347-z},
journal = {Psychometrika},
year = {2014},
title = {{Analyses of Model Fit and Robustness. A New Look at the PISA Scaling Model Underlying Ranking of Countries According to Reading Literacy}},
volume = {79},
number = {2},
pages = {210-231}
}
@article{Asil2016,
author = {Asil, Mustafa and Brown, Gavin T. L.},
title = {{Comparing OECD PISA Reading in English to Other Languages: Identifying Potential Sources of NonInvariance}},
journal = {International Journal of Testing},
year = {2016},
volume = {16},
number = {1},
pages = {71-93},
doi = {10.1080/15305058.2015.1064431}
}
@article{Veenhoven2012,
author = {Veenhoven, Ruut},
year = {2012},
title = {{Cross-national differences in happiness: Cultural measurement bias or effect of culture?}},
journal = {International Journal of Wellbeing},
volume = {2},
number = {4},
pages = {333-353},
doi = {10.5502/ijw.v2.i4.4}
}

@article{ESStopline5,
title = {{Europeans' Personal and Social Wellbeing: Topline Results from Round 6 of the European Social Survey}},
author = {Jeffrey, K and Abdallah, S and Quick, A},
journal = {ESS Topline Results (Series 5)},
year = {2015}
}

@article{Rodriguez2014,
author = {Rodr\'iguez-Pose, Andr\'es and von Berlepsch, Viola},
title = {{Social Capital and Individual Happiness in Europe}},
journal = {Journal of Happiness Studies},
year = {2014},
volume = {15},
number = {2},
pages = {357-386},
doi = {10.1007/s10902-013-9426-y}
}
@article{WHR2016,
title={{World Happiness Report 2016 Update}},
author={Helliwell, J. and Layard, R. and Sachs, J.},
journal={New York: Sustainable Development Solutions Network},
year={2016}
}


@article{Lolle2016,
author= {Lolle, Henrik Lauridsen and Andersen, J{\o}rgen Goul},
title={{Measuring Happiness and Overall Life Satisfaction: A Danish Survey Experiment on the Impact of Language and Translation Problems}},
journal={Journal of Happiness Studies},
year={2016},
volume={17},
number={4},
pages={1337-1350},
abstract={The paper addresses language and translation problems in the most typical measures of happiness and overall life satisfaction in international surveys using an experimental design. In the experiment, randomly selected groups of Danish university students answered questionnaires in English and Danish, respectively. We found significant differences in the answers on both indices. As such, it was confirmed that the term ``happy'' is not the same in English and Danish. In Danish the word is similar to the German word ``gl{\"u}cklich'' which seems to refer to something stronger than just being ``happy''. Perhaps more surprisingly, we also found a significant difference between the answers on ``overall life satisfaction'', indicating that the answers given in Danish are too high as compared to the English ones. The differences are large enough to argue that such simple tests should be conducted before ranking countries in terms of these two well-established indices of subjective well-being.},
issn={1573-7780},
doi={10.1007/s10902-015-9646-4},
url={http://dx.doi.org/10.1007/s10902-015-9646-4}
}

@article{Cattell1966,
author = {Cattell, Raymond B.},
doi = {10.1207/s15327906mbr0102_10},
issn = {0027-3171},
journal = {Multivariate Behavioral Research},
month = {apr},
number = {2},
pages = {245--276},
title = {{The Scree Test For The Number Of Factors}},
url = {http://www.tandfonline.com/doi/abs/10.1207/s15327906mbr0102{\_}10},
volume = {1},
year = {1966}
}

@article{Ekstrøm2014,
author = {Ekstr{\o}m, Claus Thorn},
doi = {10.1111/test.12027},
file = {:C$\backslash$:/Dropbox/pdf/litt/test12027.pdf:pdf},
issn = {0141982X},
journal = {Teaching Statistics},
month = {mar},
number = {1},
pages = {23--26},
title = {{Teaching 'Instant Experience' with Graphical Model Validation Techniques}},
url = {http://doi.wiley.com/10.1111/test.12027},
volume = {36},
year = {2014}
}
